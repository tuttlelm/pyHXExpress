{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "hdx_ms_hxexpressformat_analysis v3.0a       LMT         18September2023\n",
    "Modifying from _GuttmanData_ to read any dataset formatted like HXExpress ouput\n",
    "*.fasta file within data folder \n",
    "            sample   peptide_range  peptide    charge  dataformat\n",
    "file name: HSPB1_B1B5_0001-0011-MTERRVPFSLL-z2-allspectra.xlsx\n",
    "\n",
    "file contents, replicates indicated by .001 etc difference in time point header\n",
    "undeut\t\t        TD\t\t            5 sec\t\t            5.001 sec\t\t        5.002 sec\n",
    "672.3711\t0\t    672.3762\t0\t    672.3644\t0\t        672.3586\t0\t        672.3611    0\n",
    "672.3824\t41.1048\t672.3875\t321.42\t672.3757\t1515.9935\t672.3699\t294.3263\t672.3725    1132.8987\n",
    "\n",
    "\n",
    "---\n",
    "hdx_ms_GuttmanData_analysis v3.0a           LMT         01September2023\n",
    "\n",
    "the desired output from this script is the relative Deuterium level at each time point as determined from the Centroid and Binomial fits\n",
    "    and the populations if multiple states are prese sdfsadf\n",
    "\n",
    "--- ADDING DATA FORMAT TO READ IN (A different version handles complete SpecExport folder heirarchy from HDExaminer) ----\n",
    "Data from Guttman is formatted for HX-Express excel addon\n",
    "I added charge and peptide as top two entries and saved as .csv files\n",
    "data is then m/z vs I columns for each time point, un = undeut, TD = total deut, other times are \"<value> <unit>\"\n",
    "\n",
    "charge,      <value>\n",
    "peptide,     <string>\n",
    "un (m/z) ,   (I),         time2,   (I),      time3, (I)\n",
    "\n",
    "-----\n",
    "\n",
    "Version 3.0 is attempting to fit all fractions for polymodal instead of leaving the final frac to be 1 - sum(fracs)\n",
    "and adding in a 'Bootstrap' method for parameter error calculation. Bootstrap does one round of curve_fit and estimates a stderr\n",
    "based on the residuals. This error is used to generate y_new = y + (value picked from stderr distribution) X Nboot times\n",
    "the fit values are averaged and the errors in each parameter are determined from the stdev of the Nboot values\n",
    "based on https://stackoverflow.com/questions/14581358/getting-standard-errors-on-fitted-parameters-using-the-optimize-leastsq-method-i\n",
    "\n",
    "\n",
    "Version 2.0 has been updated to account for the isotopic contributions from non-exchanging atoms (e.g. 13C, 15N, and beyond)\n",
    "To do this an existing routine called 'brainpy' has been implemented to calculate the expected UnDeut Envelope\n",
    "This envelope is then nested into the NHex binomial fits \n",
    "\n",
    "see # http://mobiusklein.github.io/brainpy/docs/_build/html/#supporting-objects which is a python rewrite of BRAIN\n",
    "https://pubs.acs.org/doi/10.1007/s13361-013-0796-5 Baffling Recursive Algorithm for Isotopic distributioN calculations\n",
    "\n",
    "#!pip install brain-isotopic-distribution\n",
    "#!pip install pyteomics\n",
    "\n",
    "The input is a composition dictionary, the script currently handles standard amino acids read from the peptide sequence\n",
    "PTMs could easily be added by defining their compositions manually (using pyteomics)\n",
    "#mass.std_aa_comp['p'] = mass.Composition({'H':1,'P':1,'O':3})\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "#import matplotlib.colors as mcolors\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from scipy.optimize import curve_fit\n",
    "#from scipy.special import comb\n",
    "from math import gamma \n",
    "from pyteomics import mass\n",
    "from brainpy import isotopic_variants\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from Bio import SeqIO\n",
    "\n",
    "#!pip install openpyxl\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter (action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "    params_dir = os.path.split(PARAMS_FILE)\n",
    "    os.chdir(params_dir[0])\n",
    "    para_file = params_dir[1].rsplit('.')[0]\n",
    "\n",
    "    #p,m = PARAMS_FILE.rsplit('.',1)\n",
    "    mod = importlib.import_module(para_file)\n",
    "    importlib.reload(mod)\n",
    "\n",
    "    globals().update({v: getattr(mod, v)\n",
    "                    for v in mod.__dict__\n",
    "                    if not v.startswith(\"_\")})\n",
    "    return\n",
    "\n",
    "def write_parameters(write_dir):\n",
    "    all_params = ['Boot_Seed', 'Bootstrap', 'Data_DIR', 'Data_Type', 'Env_limit', 'Env_threshold', 'Full_boot', 'Hide_Figure_Output', \n",
    "    'Keep_Raw', 'Limit_by_envelope', 'Max_Pops', 'Metadf_File', 'Nboot', 'Ncurve_p_accept', 'Overlay_reps', 'Pop_Thresh',\n",
    "    'process_ALL', 'setNoise', 'Random_Seed', 'Read_meta', 'SVG', 'Scale_Y_Values', 'Test_Data', 'User_mutants', 'User_peptides', 'Y_ERR',\n",
    "    'WRITE_PARAMS']\n",
    "    write_file = os.path.join(write_dir,\"hdxms_params_\"+date+\".py\")\n",
    "    add_string=\"\"\n",
    "    i = 0\n",
    "    while os.path.exists(write_file):\n",
    "        i += 1\n",
    "        add_string=str(i)\n",
    "        write_file = os.path.join(Data_DIR,\"hdxms_params_\"+date+\"_\"+add_string+\".py\")\n",
    "    \n",
    "    with open(write_file,\"w\") as p_file:\n",
    "        for p in all_params:\n",
    "            #print(p,globals().get(p))\n",
    "            if type(vars().get(p)) == str:\n",
    "                p_file.write(p+\" = \\\"\"+str(globals().get(p))+\"\\\"\\n\")\n",
    "            else: p_file.write(p+\" =\"+str(globals().get(p))+\"\\n\")\n",
    "\n",
    "## Functions to read data in different formats\n",
    "\n",
    "def get_hxexpress_meta(hx_file):\n",
    "    labels = hx_file.split('-')\n",
    "    metadata = {}\n",
    "    metadata['file']=hx_file\n",
    "    metadata['sample'] = str(labels[0]).rsplit('_',1)[0] #split off the peptide start_seq\n",
    "    start_seq = str(labels[0]).rsplit('_',1)[-1] \n",
    "    end_seq = labels[-4] #end sequence is fourth from end, allows for whatever sample name formatting\n",
    "    metadata['start_seq'] = int(start_seq)\n",
    "    metadata['end_seq'] = int(end_seq)\n",
    "    metadata['peptide_range'] = '-'.join([start_seq,end_seq])\n",
    "    metadata['charge']=float(labels[-2][1:])\n",
    "    metadata['peptide'] = labels[-3]\n",
    "    return metadata\n",
    "\n",
    "def get_metadf():\n",
    "    if Data_Type == 1:\n",
    "        if Test_Data: print() ###TODO TODO TODO###    \n",
    "        else:\n",
    "            hx_files = [ f for f in os.listdir(Data_DIR) if f[-5:]=='.xlsx'  ] \n",
    "            if not process_ALL:\n",
    "                if 'all' not in User_mutants[0].lower():\n",
    "                    #available_mutants = set([um for hx in hx_files for um in User_mutants if um in hx])     \n",
    "                    hx_files = [hx for um in User_mutants for hx in hx_files if um == hx.split('-')[0].rsplit('_',1)[0]]\n",
    "                if 'all' not in User_peptides[0].lower():\n",
    "                    #available_charges = set([up for hx in hx_files for up in User_peptides if up in hx])\n",
    "                    hx_files = [hx for up in User_peptides for hx in hx_files if up in hx]\n",
    "\n",
    "\n",
    "            #HSPB1_B1B5_0001-0011-MTERRVPFSLL-z2-allspectra.xlsx\n",
    "            metadf = pd.DataFrame() #dataframe to hold filenames and sample/peptide/charge info\n",
    "            for f in hx_files:\n",
    "                meta = get_hxexpress_meta(f)\n",
    "                metadf = metadf.append(meta,ignore_index=True)\n",
    "\n",
    "    elif Data_Type == 2:\n",
    "        fasta_files = [ f for f in os.listdir(Data_DIR) if f[-6:]=='.fasta'  ]\n",
    "        if len(fasta_files)==0: print(\"no fasta files found\")\n",
    "        mutants = [ff.split('.')[0] for ff in fasta_files]\n",
    "        mutant_dirs = [os.path.split(f)[-1] for f in os.scandir(os.path.join(Data_DIR)) if f.is_dir()]\n",
    "        mutants = [o for o in mutant_dirs if o in mutants]  #ignore any extra fasta files\n",
    "\n",
    "        if (not process_ALL) and (User_mutants[0].lower() != 'all'): \n",
    "            check = all(item in mutants for item in User_mutants)\n",
    "            if not check:             \n",
    "                missing = list(set(User_mutants)-set(mutants))\n",
    "                remaining = list(set(User_mutants)-set(missing))\n",
    "                print(\"missing fasta files for: \", *list(set(User_mutants)-set(mutants)))      \n",
    "                mutants = [o for o in User_mutants if o in remaining]\n",
    "                if len(mutants): print(\"only processing\",*mutants)\n",
    "            else: mutants = User_mutants\n",
    "\n",
    "        smeta = {}\n",
    "        metadf = pd.DataFrame()\n",
    "        for mutant in mutants:\n",
    "            peptide_dirs = [f.path for f in os.scandir(os.path.join(Data_DIR,mutant)) if f.is_dir()]\n",
    "            peptide_ids = [os.path.split(ff)[-1] for ff in peptide_dirs]  \n",
    "            peptide_ranges = [ff.rsplit('-',1)[0] for ff in peptide_ids]\n",
    "            if (not process_ALL) and (User_peptides[0].lower() != 'all'): \n",
    "                check = all(item in peptide_ranges for item in User_peptides)\n",
    "                if not check:\n",
    "                    print(\"missing peptides for: \",mutant, *list(set(User_peptides)-set(peptide_ranges)))\n",
    "                    missing = list(set(User_peptides)-set(peptide_ranges))\n",
    "                    remaining = list(set(User_peptides)-set(missing))\n",
    "                    peptide_ranges = [o for o in User_peptides if o in remaining]\n",
    "                else: \n",
    "                    peptide_ranges = User_peptides\n",
    "                peptide_dirs = list(filter(lambda x: any(userpep in x for userpep in User_peptides),peptide_dirs))\n",
    "            \n",
    "            fasta_sequence =  SeqIO.parse(open(os.path.join(Data_DIR,str(mutant)+'.fasta')),'fasta')\n",
    "            for fasta in fasta_sequence:\n",
    "                sequence = str(fasta.seq)\n",
    "\n",
    "            for spec_dir,peptide_range in zip(peptide_dirs,peptide_ranges):\n",
    "                csv_files = [ f for f in os.listdir(spec_dir) if f[-4:]=='.csv'  ]\n",
    "                charges = set([int(c[-5]) for c in csv_files if c[-5].isdigit() and c[-6]=='z'])\n",
    "                for charge in charges:\n",
    "                    smeta['file']=os.path.split(spec_dir)[1]\n",
    "                    smeta['sample']=os.path.split(os.path.split(spec_dir)[0])[1]\n",
    "                    smeta['charge']=float(charge)\n",
    "                    smeta['peptide_range']=peptide_range\n",
    "                    startseq = int(peptide_range.split('-')[0])\n",
    "                    endseq = int(peptide_range.split('-')[1])\n",
    "                    smeta['start_seq'] = startseq\n",
    "                    smeta['end_seq'] = endseq\n",
    "                    smeta['peptide'] = sequence[startseq-1:endseq]\n",
    "                    metadf = metadf.append(smeta, ignore_index = True)   \n",
    "    else: print(\"Must specify Data_Type: 1 for HX-Express allspectra format or 2 for SpecExport\")\n",
    "    if metadf.empty: raise Exception(\"No data found. Check Data_Type specification and Data_DIR\")\n",
    "    else:\n",
    "        metadf = metadf.sort_values(['peptide_range','sample','charge',],ignore_index=True) \n",
    "        print(\"Found\",len(metadf['sample'].unique()),\"sample types with\",len(metadf),\"total datasets to analyze.\")\n",
    "    return metadf\n",
    "\n",
    "#safety function in case peptide sequence is bad\n",
    "def goodseq(seq):\n",
    "    try: \n",
    "        mass.most_probable_isotopic_composition(sequence=seq)\n",
    "        return True\n",
    "    except: \n",
    "        print(f\"Sequence {seq} is not defined\")\n",
    "        #exit()\n",
    "        return False\n",
    "\n",
    "def read_hexpress_data(f,keep_raw = False):\n",
    "    \n",
    "    raw=[]\n",
    "    all_raw = pd.DataFrame()\n",
    "    dfs = pd.DataFrame()\n",
    "    deutdata = pd.DataFrame()\n",
    "    peaks=[]\n",
    "\n",
    "    #for f in file_list:    \n",
    "    if Test_Data:\n",
    "        split_name = f.lower().split('_')\n",
    "        if any('angio' in x for x in split_name):\n",
    "            sample = 'Angio'\n",
    "            start_seq = '1' \n",
    "            end_seq = '8'\n",
    "            peptide_range = '-'.join([start_seq,end_seq])\n",
    "            charge=2.0\n",
    "            peptide = 'DRVYIHPF'              \n",
    "        elif any('glufib' in x for x in split_name):\n",
    "            sample = 'GluFib'\n",
    "            start_seq = '1' \n",
    "            end_seq = '14'\n",
    "            peptide_range = '-'.join([start_seq,end_seq])\n",
    "            charge=2.0\n",
    "            peptide = 'EGVNDNEEGFFSAR' \n",
    "        else: \n",
    "            print(\"test dataset not found\")\n",
    "            return\n",
    "    else:  #HSPB1_B1B5_0001-0011-MTERRVPFSLL-z2-allspectra.xlsx\n",
    "        labels = f.split('-')\n",
    "        sample = str(labels[0]).rsplit('_',1)[0] #split off the peptide start_seq\n",
    "        start_seq = str(labels[0]).rsplit('_',1)[-1] \n",
    "        end_seq = labels[-4] #end sequence is fourth from end, allows for whatever sample name formatting\n",
    "        peptide_range = '-'.join([start_seq,end_seq])\n",
    "        charge=float(labels[-2][1:])\n",
    "        peptide = labels[-3]\n",
    "\n",
    "    file=os.path.join(Data_DIR, f)\n",
    "        \n",
    "    try: timepts = pd.read_excel(file,header=None,nrows=1) #get headers\n",
    "    except IOError as e:\n",
    "        print (f\"Could not read: {f} check path or if file is open\")\n",
    "        return deutdata\n",
    "    #print(timepts)\n",
    "    times =[x for x in timepts.values[0] if str(x) != 'nan']\n",
    "    # print(times)\n",
    "\n",
    "    delays = []\n",
    "    for i,dtime in enumerate(times):\n",
    "        rep = 1     \n",
    "        if dtime[0:6] == 'undeut': delay = 0.0 \n",
    "        elif dtime[0:2] == 'TD': delay = 1e6\n",
    "        else:\n",
    "            tp = float(dtime.split(' ')[0].split('.')[0])\n",
    "            tunit = dtime.split(' ')[-1]\n",
    "            #print(tp, tunit)\n",
    "            delay = tp * np.power(60.0,'smh'.find(tunit[0]))\n",
    "        delays += [delay]\n",
    "        rep = Counter(delays)[delay]\n",
    "        \n",
    "        # #print(i,time)\n",
    "        raw = pd.read_excel(file,skiprows=1,header=None,usecols=[i*2,i*2+1],names=['mz','Intensity']).dropna()\n",
    "        peaks = peak_picker( raw, peptide, charge, count_sc=0)\n",
    "        peaks['time']=delay\n",
    "        peaks['sample']=sample\n",
    "        peaks['peptide']=peptide\n",
    "        peaks['charge']=charge\n",
    "        peaks['rep']=rep\n",
    "        peaks['peptide_range']=peptide_range\n",
    "        if keep_raw:\n",
    "            raw['time']=delay\n",
    "            raw['sample']=sample\n",
    "            raw['peptide']=peptide\n",
    "            raw['charge']=charge\n",
    "            raw['rep']=rep\n",
    "            raw['peptide_range']=peptide_range\n",
    "            all_raw = pd.concat([all_raw,raw])\n",
    "        dfs = pd.concat([dfs,peaks],ignore_index=True)\n",
    "\n",
    "    deutdata = dfs.copy()\n",
    "\n",
    "    time_points = sorted(set(deutdata.time))\n",
    "    #n_time_points = len(time_points)\n",
    "\n",
    "    deutdata['time_idx'] = [ time_points.index(t) for t in deutdata.time ]\n",
    "\n",
    "    ## read in solution file\n",
    "    if Test_Data:\n",
    "        file=os.path.join(Data_DIR, \"bimodal_solutions.txt\")\n",
    "        solution = pd.read_csv(file,delim_whitespace=True,)\n",
    "        solution = solution.sort_values('time').reset_index()\n",
    "        all_raw['time_idx'] = [ time_points.index(t) for t in all_raw.time ]\n",
    "        return deutdata, all_raw, solution\n",
    "    elif keep_raw: \n",
    "        all_raw['time_idx'] = [ time_points.index(t) for t in all_raw.time ]\n",
    "        return deutdata, all_raw\n",
    "    else: return deutdata\n",
    "\n",
    "def get_na_isotope(peptide,charge):\n",
    "    pepcomp = {}\n",
    "    na_isotope=[]\n",
    "    if goodseq(peptide): comp = mass.Composition(peptide) \n",
    "    else: comp = {}\n",
    "    for key in list(comp):\n",
    "        pepcomp[key] = comp[key]\n",
    "    pepcomp['H'] = pepcomp['H']-count_amides(peptide,count_sc=0.0)\n",
    "    #pepcomp = {'H': 53, 'C': 34, 'O': 15, 'N': 7}\n",
    "    theoretical_isotopic_cluster = isotopic_variants(pepcomp, npeaks=10, charge=charge)\n",
    "\n",
    "    for ipeak in theoretical_isotopic_cluster:\n",
    "        na_isotope = np.append(na_isotope, ipeak.intensity) #for now just make continuous list for all peptides\n",
    "                                                            #won't make sense to do this for full version\n",
    "    return na_isotope\n",
    "\n",
    "def count_amides (peptide,count_sc=0.0):\n",
    "    ex_sc = 0\n",
    "    proline = peptide[1:].count('P')\n",
    "    for sidechain in 'STYCDEHW':\n",
    "        ex_sc += peptide.count(sidechain)\n",
    "    for sidechain in 'R':\n",
    "        ex_sc += 2*peptide.count(sidechain)\n",
    "    for sidechain in 'KQN':\n",
    "        ex_sc += 2*peptide.count(sidechain)\n",
    "    n_amides = len(peptide)-proline-1+int(ex_sc*count_sc)\n",
    "    return n_amides\n",
    "\n",
    "\n",
    "def peak_picker(data, peptide,charge,resolution=100.0,count_sc=0.0):\n",
    "    n_amides = count_amides(peptide,count_sc=1.0) #including sidechains for maximum m/z window\n",
    "    undeut_mz = mass.calculate_mass(sequence=peptide,show_unmodified_termini=True,charge=charge) \n",
    "    n_deut = np.arange(n_amides+1)\n",
    "    pred_mzs = undeut_mz + (n_deut*1.006227)/charge\n",
    "    mz_mid = pred_mzs.mean()\n",
    "\n",
    "    threshold = 0.01 * data.Intensity.max() #this should be set to some noise value based on actual data\n",
    "\n",
    "    peaks = []\n",
    "    zeroes = 0.0\n",
    "\n",
    "    for i,pred_mz in enumerate(pred_mzs):\n",
    "        #mass accuracy is in ppm, so default is 50ppm\n",
    "        mz_range = pred_mz * (1.0+np.array([-1,1])*resolution/1e6)    \n",
    "        #sort data in range, grab first entry which is the peak\n",
    "        focal_data = data.copy()[data.mz.between(*mz_range)]\n",
    "        focal_data['n_deut'] = i \n",
    "        focal_data = focal_data.sort_values('Intensity',ascending=False).reset_index(drop=True)\n",
    "\n",
    "        if (len(focal_data) > 0):\n",
    "            intensity = max(focal_data['Intensity'].max() - threshold, 0.0)\n",
    "            if (intensity == 0.0 and pred_mz > mz_mid): zeroes += 0.5 \n",
    "        else:\n",
    "            intensity = 0.0\n",
    "            if (pred_mz > mz_mid): zeroes += 1\n",
    "        peak = pd.DataFrame({'mz':[pred_mz],'Intensity':[intensity],'n_deut':[i]})\n",
    "        peaks.append( peak )\n",
    "        if zeroes > 4: break\n",
    "\n",
    "    return pd.concat(peaks,ignore_index=True)\n",
    "\n",
    "def get_mz_env(value, df, colname='Intensity',pts=False):\n",
    "    '''\n",
    "    get mz values at value = envelope_height (e.g. 0.1*maxIntensity)\n",
    "    to define the envelope width, for assessment of expected polymodal fits\n",
    "    '''\n",
    "    df = df.reset_index()\n",
    "    boolenv = df[colname].gt(value)\n",
    "    envpts = boolenv.value_counts()[True] # number of points in envelope\n",
    "    loweridx = df[colname].where(boolenv).first_valid_index()\n",
    "    upperidx = df[colname].where(boolenv).last_valid_index()\n",
    "    if df[colname].iloc[0] > value:\n",
    "        min_mz = df['mz'].iloc[0] #envelope starts with first datapoint\n",
    "        left_Int = df[colname].iloc[0] \n",
    "    else:     \n",
    "        x1a = df[colname].iloc[loweridx]\n",
    "        x1b = df[colname].iloc[loweridx-1]\n",
    "        y1a = df['mz'].iloc[loweridx]\n",
    "        y1b = df['mz'].iloc[loweridx-1]\n",
    "        min_mz = y1a + (y1b - y1a) * (value - x1a)/(x1b - x1a)\n",
    "        left_Int = value\n",
    "    \n",
    "    if df[colname].iloc[-1] > value:\n",
    "        max_mz = df['mz'].iloc[-1] #envelope goes to end of datapoints, poorly picked masspec?\n",
    "    else:\n",
    "        x2a = df[colname].iloc[upperidx]\n",
    "        x2b = df[colname].iloc[upperidx+1]\n",
    "        y2a = df['mz'].iloc[upperidx]\n",
    "        y2b = df['mz'].iloc[upperidx+1]\n",
    "        max_mz = y2a + (y2b - y2a) * (value - x2a)/(x2b - x2a)\n",
    "    \n",
    "    if pts: return np.array([min_mz, max_mz]), left_Int\n",
    "    else: return np.array([min_mz, max_mz])\n",
    "\n",
    "## Multi-binomial Functions\n",
    "def nCk_real(n,k):\n",
    "    #print(\"n,k:\",n,k)\n",
    "    if n - k + 1 <= 0: return 0.0\n",
    "    #elif n+k > 50: return 1e10 #this breaks things, would need to handle overflows some other way\n",
    "    else: return gamma(n+1)/(gamma(k+1)*gamma(n-k+1))\n",
    "\n",
    "def binom(bins, n, p):\n",
    "    k = np.arange(bins+1).astype(float)\n",
    "    nCk = [nCk_real(n,y) for y in k]\n",
    "    return nCk*np.power(p,k)*np.power(1-p,n-k)\n",
    "\n",
    "def binom_isotope(bins, n,p):\n",
    "    bs = binom(bins,n,p)\n",
    "    newbs=np.zeros(len(bs) + len(Current_Isotope)+1)\n",
    "    for i in range(len(bs)):\n",
    "        for j in range(len(Current_Isotope)):     \n",
    "            newbs[i+j] += bs[i]*Current_Isotope[j]  \n",
    "    return newbs[0:bins+1]\n",
    "\n",
    "def n_binomials( bins, *params ): #allfracsversion\n",
    "    # params takes the form [scaler, n_1, n_2, mu_1, ..., mu_n, frac_1, ..., frac_n] \n",
    "    n_curves = int( (len(params)+1) / 3.0 )\n",
    "    log_scaler = params[0]\n",
    "    n_array=np.array(params[1:n_curves+1])\n",
    "    mu_array = np.array( params[n_curves+1:2*n_curves+1] )\n",
    "    frac_array=[]\n",
    "    frac_array = np.array( params[ -n_curves: ] )\n",
    "    frac_array = frac_array/np.sum(frac_array)\n",
    "    poissons = [ frac * binom( bins,n, mu ) for frac, n, mu in zip( frac_array, n_array, mu_array ) ]\n",
    "    return np.power( 10.0, log_scaler ) * np.sum( poissons, axis=0, )\n",
    "\n",
    "\n",
    "def n_binom_isotope( bins, *params ): #allfracsversion\n",
    "    # params takes the form [ scaler, mu_1, ..., mu_n, frac_1, ..., frac_n] \n",
    "    n_curves = int(( len(params) + 1) / 3.0 )\n",
    "    log_scaler = params[0]\n",
    "    n_array = np.array( params[1:n_curves+1] )\n",
    "    mu_array = np.array( params[n_curves+1:2*n_curves+1] )\n",
    "    frac_array = np.array( params[ -n_curves: ] )\n",
    "    frac_array = frac_array/np.sum(frac_array)\n",
    "    poissons = [ frac * binom_isotope( bins, n, mu ) for frac, n, mu in zip( frac_array, n_array, mu_array ) ]\n",
    "    truncated = np.power( 10.0, log_scaler ) * np.sum( poissons, axis=0, )[0:bins+1]\n",
    "    return truncated \n",
    "\n",
    "def calc_rss( true, pred,yerr_systematic=0.0 ):\n",
    "    return np.sum( (pred-true)**2 + yerr_systematic**2 )\n",
    "\n",
    "def get_params(*fit, sort = False, norm = False, unpack = True): \n",
    "    # assuming all fracs\n",
    "    # binom eqs of form: scaler, nex * n_curves, mu * n_curves, frac * n_curves\n",
    "    # if sort then reorder nex,mu,frac in order of mu*nexs\n",
    "    # if norm, then fracs will be scaled to sum to 1.0\n",
    "        # need to be mindful of mixing up corresponding errors when sorting and norming\n",
    "\n",
    "    num_curves = int((len(fit)-1)/3)\n",
    "\n",
    "    scaler = fit[0]\n",
    "    \n",
    "    nexs = np.array(fit[1:num_curves+1])\n",
    "    mus = np.array(fit[num_curves+1:num_curves*2+1])\n",
    "    fracs = np.array(fit[-num_curves:])\n",
    "\n",
    "    if sort:\n",
    "        mn = nexs*mus\n",
    "        use_index = mn.argsort()\n",
    "        nexs = nexs[use_index]\n",
    "        mus = mus[use_index]#[::-1]]\n",
    "        fracs = fracs[use_index]#[::-1]]\n",
    "    if norm: \n",
    "        sumfracs = np.sum(fracs)\n",
    "        fracs = fracs/sumfracs\n",
    "    \n",
    "    if unpack:\n",
    "        return scaler, nexs, mus, fracs\n",
    "    else:\n",
    "        return np.concatenate((np.array([scaler]),nexs,mus,fracs))\n",
    "\n",
    "def init_params(n_curves,max_n_amides,max_y,seed=None):\n",
    "    rng=np.random.default_rng(seed=seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    log_scaler_guess = 0.0  \n",
    "    nex_guess = max_n_amides/2.0*random.uniform(0.7,1.0) #was same for all at max/2 in < ver3.0\n",
    "    nex_low = 0.0 \n",
    "    sampled = rng.random(n_curves*2) #array for both mus and fracs\n",
    "    mu_guess =  list(sampled[0:n_curves]) #[0.2, 0.5, 0.1]\n",
    "    frac_guess = sampled[-n_curves:]#[0.70, 0.25, 0.05] #allfracs\n",
    "    frac_guess = list(frac_guess/np.sum(frac_guess))\n",
    "    frac_uppers = [1.0,1.0,1.0]\n",
    "\n",
    "    initial_estimate = [ log_scaler_guess ] + [ nex_guess ] * n_curves + mu_guess[0:n_curves] + frac_guess[0:n_curves]\n",
    "    lower_bounds = [ 0.0 ] + [nex_low]* n_curves + [0.0]* n_curves*2 \n",
    "    upper_bounds = [ np.log10(max_y)+1, ] + [max_n_amides]* n_curves + [ 1.0 ] *n_curves + frac_uppers[0:n_curves]\n",
    "    bounds = ( lower_bounds, upper_bounds, )\n",
    "\n",
    "    return initial_estimate, bounds\n",
    "\n",
    "def fit_bootstrap(p0_boot, bounds, datax, datay, sigma_res=None,yerr_systematic=0.0,nboot=100,\n",
    "                  ax=None,full=False,yscale=1.0):\n",
    "    #p0_boot is a list of all p0 initial parameters with nboot entries\n",
    "    #if ax != None: ax.plot( mz, datay, color = 'cyan', linestyle='solid',label='boot_datay' )\n",
    "    p0 = p0_boot[0]\n",
    "    num_curves = int((len(p0)-1)/3)\n",
    "    \n",
    "    if sigma_res==None:\n",
    "        # Fit first time if no residuals\n",
    "        pfit, perr = curve_fit( n_fitfunc, datax, datay, p0, maxfev=int(1e6), \n",
    "                                        bounds = bounds   )\n",
    "\n",
    "        print(\"Ran initial bootstrap curve_fit to generate residuals\")\n",
    "\n",
    "        # Get the stdev of the residuals\n",
    "        residuals = n_fitfunc(datax,*pfit) - datay\n",
    "        sigma_res = np.std(residuals)\n",
    "    sigma_err_total = np.sqrt(sigma_res**2 + yerr_systematic**2)\n",
    "\n",
    "    # nboot random data sets are generated and fitted\n",
    "    ps = []\n",
    "    ps_cov = []\n",
    "    for i in range(nboot):\n",
    "        p0 = p0_boot[i]\n",
    "        randomDelta = np.random.normal(0., sigma_err_total, len(datay))\n",
    "        randomDelta = [0 if a==0 else b for a,b in zip(datay,randomDelta)] #don't change y if y=0\n",
    "        randomdataY = datay + randomDelta \n",
    "        #if datapoint is zero, leave as zero and don't let value go negative\n",
    "        randomdataY = np.clip(randomdataY, 0.0, np.inf) \n",
    "        if (len(p0) > len(randomdataY)): \n",
    "                            print(f\"attempting to fit more paramaters than data points in bootstrap\")\n",
    "                            #should be able to exit at this point, haven't updated last fit parameters including p_err\n",
    "                            break # exit the for n_curves loop\n",
    "        randomfit, randomcov = curve_fit( n_fitfunc, datax, randomdataY, p0, maxfev=int(1e6), \n",
    "                                    bounds = bounds   )\n",
    "        \n",
    "        randomfit[0] = np.power( 10.0, randomfit[0] )\n",
    "        #rfit = randomfit\n",
    "        rfit = get_params(*randomfit,sort=True,norm=True,unpack=False) #randomfit #\n",
    "        ps.append(rfit)\n",
    "        ps_cov.append(randomcov) ## this won't work if ever Sorting\n",
    "\n",
    "        #ax.plot( mz, randomdataY*yscale, color = 'magenta', linestyle='dashed', )\n",
    "        if ax != None: \n",
    "            # ax.plot( mz, randomdataY, color = 'green', linestyle='dashed', )\n",
    "            tempr = rfit.copy()\n",
    "            tempr[0] = np.log10(rfit[0])\n",
    "            boot_y = n_fitfunc(datax, *tempr) * yscale \n",
    "            ax.plot( mz, boot_y, color = 'darkviolet', linestyle='solid', alpha=0.2 )#label='bootstraps',zorder=10)\n",
    "            s,n,m,f = get_params(*rfit,norm=True,unpack=True)\n",
    "            for k in range( num_curves ):\n",
    "                bfit_yk = s * f[k] * fitfunc( datax, n[k], m[k], ) * yscale\n",
    "                #plot_label = ('pop'+str(k+1)+' = '+format(frac,'.2f')+'\\nNex'+str(k+1)+' = '+format(nex,'.1f'))\n",
    "                ax.plot( mz, bfit_yk, color = 'green', linestyle='dashed',linewidth=3,alpha=0.2)#label=plot_label)\n",
    "\n",
    "\n",
    "    ps = np.array(ps)\n",
    "    mean_pfit = np.mean(ps,axis=0)\n",
    "\n",
    "    # You can choose the confidence interval that you want for your\n",
    "    # parameter estimates: \n",
    "    Nsigma = 1. # 1sigma corresponds to 68.3% confidence interval\n",
    "                # 2sigma corresponds to 95.44% confidence interval\n",
    "    ps[:,0] = np.log10(ps[:,0])\n",
    "    err_pfit = Nsigma * np.std(ps,axis=0)\n",
    "\n",
    "    mean_pfit[0] = np.log10(mean_pfit[0])\n",
    "\n",
    "    pfit_bootstrap = mean_pfit\n",
    "    perr_bootstrap = err_pfit\n",
    "    if Full_boot: return ps, ps_cov #return all the bootstrap fits \n",
    "    else: return pfit_bootstrap, perr_bootstrap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "'''begin user input'''\n",
    "##########################################\n",
    "\n",
    "USE_PARAMS_FILE = False  #### IF THIS IS TRUE ALL PARAMETERS ARE READ FROM PARAMS_FILE:\n",
    "PARAMS_FILE = '/home/tuttle/data/HDX-MS/Pearl_SpecExport_30oct2023/SpecExport/hdxms_params.py'\n",
    "\n",
    "## OR if USE_PARAMS_FILE = False *** COMPLETE THE FOLLOWING SECTION *** ##\n",
    "WRITE_PARAMS = True #save the params to hdxms_params_$.py file in Data_DIR, can then be read in as PARAMS_FILE \n",
    "\n",
    "Data_Type = 2  \n",
    "    #1: 'xlxs' , each file contains all timepoint reps at Data_DIR/onesample_onepeptide_alltimepts_allreps_onecharge.xlsx\n",
    "                # current recognized format is e.g. HSPB1_B1B5_0001-0011-MTERRVPFSLL-z2-allspectra.xlsx\n",
    "                # <sample name>_<peptide start>-<peptide end>-<peptide>-<zcharge>-<unused label>.xlsx\n",
    "                # allows for replicats of UnDeut/TD even though HX-Express xlsm does not\n",
    "    #2: 'SpecExport', as exported from HDExaminer Data_DIR/samples/peptides/onetimept_onerep_onecharge.csv\n",
    "                # this mode requires a sample.fasta file in the Data_DIR for each sample to be processed, with matching names\n",
    "\n",
    "#user_ settings only used if process_ALL = False\n",
    "#first element can be 'all' to include all mutants and/or peptides in directory\n",
    "if Data_Type == 1:\n",
    "    Data_DIR = 'c:\\\\Users\\\\tuttl\\\\OneDrive\\\\Documents\\\\My Documents\\\\KlevitHahn\\\\hdx-ms\\\\ns_HSPB1_Bimodal_Peptide_Data'\n",
    "    process_ALL = False # if True will assume all .xlsx files are HDX data, use with care\n",
    "    User_mutants = ['HSPB1only',]#'HSPB1_B1B6'] #['all']\n",
    "    User_peptides = ['0001-0011',]#'0078-0094']\n",
    "if Data_Type == 2:\n",
    "    #data_dir = '/data/tuttle/HDX-MS/Pearl_SpecExport_30oct2023/SpecExport'\n",
    "    data_dir = 'c:\\\\Users\\\\tuttl\\\\OneDrive\\\\Documents\\\\My Documents\\\\KlevitHahn\\\\hdx-ms\\\\ns_HSPB1_Bimodal_Peptide_Data\\\\SpecExport'\n",
    "    process_ALL = False #process_all = True is limited to existing .fasta files, this setting overrides user_ settings\n",
    "    user_mutants = ['all','WTmannose']#,'B1B6','HSPB1'] #['WT','S19D','S45D','S59D','D3']#['All'] #\n",
    "    user_peptides =  [ '0034-0038',]#'0049-0054']#['0034-0045'] #['0093-0116'] #['0090-0113']'0122-0166']#\n",
    "\n",
    "Test_Data = False\n",
    "if Test_Data: \n",
    "    Data_Type = 1\n",
    "    #Data_DIR = 'c:\\\\Users\\\\tuttl\\\\OneDrive\\\\Documents\\\\My Documents\\\\KlevitHahn\\\\hdx-ms\\\\HX-Express3'\n",
    "    Data_DIR = 'C:\\\\Users\\\\tuttl\\\\OneDrive\\\\Documents\\\\My Documents\\\\KlevitHahn\\\\hdx-ms\\\\pyHXExpress\\\\Bimodal_HDX_Data'\n",
    "    #Test_Sets = ['v3_Angiotensin_Bimodals.xlsx','v3_GluFib_Bimodals.xlsx']\n",
    "    Test_Sets = ['all']\n",
    "\n",
    "Read_meta = True # Specify files to be run in a file. \n",
    "                # To use this, Recommend setting to False to create and write 'metadf' to file with all availble datasets\n",
    "                # then remove unwanted datasets from the file, and read it in with Read_meta = True\n",
    "if Read_meta: Metadf_File = \"hdx_spectra_list_metadf_02Nov2023.csv\"\n",
    "else: Metadf_File = None\n",
    "\n",
    "Hide_Figure_Output = False\n",
    "SVG = False # also save figures as an svg file, slow, but better for making figures \n",
    "\n",
    "Bootstrap = True #False #\n",
    "Full_boot=True #plot all the bootstrap fits, frac vs nex*mu\n",
    "\n",
    "Nboot = 20 # number of individual fits to perform, using n_best_curves from initial round of fits\n",
    "setNoise = False #if noise value is known, specify instead of estimating as Y_ERR % of avg Un+TD peaks\n",
    "Y_ERR = 1.0 #Percent random error applied during boot as y*+np.random.normal(0,yerr), 0.0 for NoNoise, ~0.5% for noise added\n",
    "            # the absolute Noise value is then Y_ERR * avg(maxInt of Un and TD)\n",
    "            # this is a very rough way to give a consistent Noise value throughout a dataset. \n",
    "\n",
    "Env_threshold = 0.1 #find envelope width at Env_threshold * Intensity_max\n",
    "Limit_by_envelope = False # only fit up to n = int(z*env/3*Env_limit - 2/3) \n",
    "Env_limit = 1.0 #used if Limit_by_envelope = True, rough measure to constrain n_curves fit according to data width & num fit params\n",
    "Max_Pops = 2 #maximum number of underlying populations to fit\n",
    "Pop_Thresh = 0.03 #fall back to n-1 curves if population is below this, does not apply to bootstrap fits, but does exclude from boot average values\n",
    "Ncurve_p_accept = 0.02 #stringency for accepting more fit populations      \n",
    "Random_Seed = 16 #used for parameter initialization\n",
    "Boot_Seed = True #if False, same seed as Random_Seed, \n",
    "                 #otherwise different seed for each boot iteration (0 to Nboot + Random_Seed + 1 to not repeat initial fit)   \n",
    "Scale_Y_Values = True # if Scale_Y_Values = True, plots will be in original Intensity units\n",
    "                # fit will always be on normalized Intensity as it is much faster               \n",
    "Keep_Raw = True # peak_picking will retain the Raw spectrum if True, if False will only keep peaks, auto True for Test_Data\n",
    "Overlay_reps = True #add column to figures that is overlay of all available replicates\n",
    "\n",
    "########################################\n",
    "'''end user input''';\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "date = now.strftime(\"%d%b%Y\")\n",
    "USE_PARAMS_FILE = True\n",
    "PARAMS_FILE = '/home/tuttle/data/HDX-MS/Pearl_SpecExport_30oct2023/SpecExport/hdxms_params.py'\n",
    "\n",
    "\n",
    "def get_parameters():\n",
    "    params_dir = os.path.split(PARAMS_FILE)\n",
    "    os.chdir(params_dir[0])\n",
    "    para_file = params_dir[1].rsplit('.')[0]\n",
    "\n",
    "    #p,m = PARAMS_FILE.rsplit('.',1)\n",
    "    mod = importlib.import_module(para_file)\n",
    "    importlib.reload(mod)\n",
    "\n",
    "    globals().update({v: getattr(mod, v)\n",
    "                    for v in mod.__dict__\n",
    "                    if not v.startswith(\"_\")})\n",
    "    return\n",
    "\n",
    "def write_parameters(write_dir):\n",
    "    all_params = ['Boot_Seed', 'Bootstrap', 'Data_DIR', 'Data_Type', 'Env_limit', 'Env_threshold', 'Full_boot', 'Hide_Figure_Output', \n",
    "    'Keep_Raw', 'Limit_by_envelope', 'Max_Pops', 'Metadf_File', 'Nboot', 'Ncurve_p_accept', 'Overlay_reps', 'Pop_Thresh',\n",
    "    'process_ALL', 'setNoise', 'Random_Seed', 'Read_meta', 'SVG', 'Scale_Y_Values', 'Test_Data', 'User_mutants', 'User_peptides', 'Y_ERR',\n",
    "    'WRITE_PARAMS']\n",
    "    write_file = os.path.join(write_dir,\"hdxms_params_\"+date+\".py\")\n",
    "    add_string=\"\"\n",
    "    i = 0\n",
    "    while os.path.exists(write_file):\n",
    "        i += 1\n",
    "        add_string=str(i)\n",
    "        write_file = os.path.join(Data_DIR,\"hdxms_params_\"+date+\"_\"+add_string+\".py\")\n",
    "    \n",
    "    with open(write_file,\"w\") as p_file:\n",
    "        for p in all_params:\n",
    "            #print(p,globals().get(p))\n",
    "            if type(vars().get(p)) == str:\n",
    "                p_file.write(p+\" = \\\"\"+str(globals().get(p))+\"\\\"\\n\")\n",
    "            else: p_file.write(p+\" =\"+str(globals().get(p))+\"\\n\")\n",
    "\n",
    "\n",
    "if USE_PARAMS_FILE:\n",
    "    get_parameters()\n",
    "if WRITE_PARAMS:\n",
    "    write_parameters(Data_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN IT \n",
    "now = datetime.now()\n",
    "date = now.strftime(\"%d%b%Y\")\n",
    "\n",
    "if USE_PARAMS_FILE:\n",
    "    get_parameters()\n",
    "if WRITE_PARAMS:\n",
    "    write_parameters(Data_DIR)\n",
    "\n",
    "\n",
    "mpl_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "mpl_colors_dark = ['#005794', '#df5f0e', '#0c800c', '#b60708', '#74479d', '#6c362b', '#b357a2', '#5f5f5f', '#9c9d02', '#079eaf']\n",
    "mpl_colors_light = ['#2f97e4', '#ff9f2e', '#4cc04c', '#f64748', '#b487ed', '#ac766b', '#f397e2', '#9f9f9f', '#dcdd42', '#37deef']\n",
    "mpl_colors_light2 = ['#4fb7f4', '#ffbf4e', '#6ce06c', '#f66768', '#d4a7fd', '#cc968b', '#f3b7f2', '#bfbfbf', '#fcfd62', '#57feff']\n",
    "colors2d = [mpl_colors_dark, mpl_colors, mpl_colors_light,mpl_colors_light2]\n",
    "\n",
    "fout = open(os.path.join(Data_DIR,'output_v3allfracs_'+date+'.txt'),'w')\n",
    "\n",
    "n_fitfunc = n_binom_isotope # n_binomials #\n",
    "fitfunc = binom_isotope # binom #\n",
    "fitfunc_name=str(fitfunc).split()[1]  #fit function as string to add to filenames\n",
    "\n",
    "## Get mutant/peptide/charge info from all datasets ##\n",
    "if Read_meta: metadf = pd.read_csv(os.path.join(Data_DIR,Metadf_File)).drop('Index',axis=1)\n",
    "else: metadf = get_metadf()\n",
    "\n",
    "### end get metadata ###\n",
    "\n",
    "### Process all the Data ###\n",
    "for index, row in metadf.iterrows():\n",
    "    rawdata = pd.DataFrame(None) \n",
    "    deutdata = pd.DataFrame(None) \n",
    "\n",
    "    hdx_file = row['file']\n",
    "    sample = row['sample']\n",
    "    peptide = row['peptide']\n",
    "    charge = row['charge']\n",
    "    peptide_range = row['peptide_range']\n",
    "    \n",
    "    if Data_Type == 1:\n",
    "        if Test_Data:\n",
    "            Keep_Raw = True\n",
    "            deutdata, rawdata, solution = read_hexpress_data(hdx_file,keep_raw=Keep_Raw)\n",
    "        elif Keep_Raw:\n",
    "            deutdata, rawdata = read_hexpress_data(hdx_file,keep_raw=Keep_Raw)\n",
    "        else: deutdata = read_hexpress_data(hdx_file,keep_raw=Keep_Raw)\n",
    "    else: # Data_type == 2, already checked that it is 1 or 2\n",
    "        dfs = []\n",
    "        raw = []\n",
    "        spec_path = os.path.join(Data_DIR,row['sample'],row['file'])\n",
    "        csv_files = [ f for f in os.listdir(spec_path) if f[-5:]==str(int(charge))+'.csv'  ]\n",
    "\n",
    "        for f in csv_files:\n",
    "            fileinfo = f.split('.')[0].split('-')\n",
    "            # print(fileinfo, len(fileinfo))\n",
    "            rep = float(fileinfo[-2])\n",
    "            if fileinfo[0] == 'Non': time = 0.0 \n",
    "            elif fileinfo[0] == 'Full': time = 1e6\n",
    "            else: time = float(fileinfo[0][:-1]) * np.power(60.0,'smh'.find(fileinfo[0][-1]))\n",
    "            raw = pd.read_csv( os.path.join(spec_path,f),delimiter=\",\",header=None, names=[\"mz\",\"Intensity\"]).dropna()\n",
    "            peaks = peak_picker( raw, peptide, charge )\n",
    "            peaks['time']=time\n",
    "            peaks['rep']=rep\n",
    "            peaks['sample']=sample\n",
    "            peaks['charge']=charge\n",
    "            peaks['peptide']=peptide\n",
    "            if peaks.Intensity.sum() > 0:\n",
    "                dfs.append( peaks )         \n",
    "            else: print (\"File \"+f+\" contains no Intensity data at expected m/z values\")\n",
    "            if Keep_Raw:\n",
    "                raw['time']=time\n",
    "                raw['sample']=sample\n",
    "                raw['peptide']=peptide\n",
    "                raw['charge']=charge\n",
    "                raw['rep']=rep\n",
    "                raw['peptide_range']=peptide_range\n",
    "                rawdata = pd.concat([rawdata,raw])\n",
    "        if len(dfs) > 0: \n",
    "            deutdata = pd.concat(dfs, ignore_index=True,)\n",
    "            time_points = sorted(set(deutdata.time))\n",
    "            n_time_points = len(time_points)\n",
    "            deutdata['time_idx'] = [ time_points.index(t) for t in deutdata.time ]\n",
    "            deutdata['charge'] = charge\n",
    "            \n",
    "        if Keep_Raw:\n",
    "            time_points = sorted(set(rawdata.time)) \n",
    "            rawdata['time_idx'] = [ time_points.index(t) for t in rawdata.time ]\n",
    "        \n",
    "\n",
    "    ## Now have deutdata, rawdata from any data format \n",
    "\n",
    "    if deutdata.empty:\n",
    "        print(\"No intensity data for \"+str(sample)+' peptide '+peptide_range+' z= '+str(charge))\n",
    "        continue\n",
    "    if deutdata.Intensity.sum() == 0: \n",
    "        print(\"No intensity data for \"+str(sample)+' peptide '+peptide_range+' z= '+str(charge))\n",
    "        continue\n",
    "\n",
    "#    samples = sorted(set(deutdata[\"sample\"])) #this only contains one sample as implemented \n",
    "#    sample = samples[0]    \n",
    "    time_points = sorted(set(deutdata.time))\n",
    "    n_time_points = len(time_points)\n",
    "    max_time_reps = int(sorted(set(deutdata.rep))[-1])\n",
    "    #peptide = deutdata['peptide'].unique()[0]\n",
    "    #peptide_range = deutdata['peptide_range'].unique()[0]\n",
    "    #charge = deutdata['charge'].unique()[0]\n",
    "    Current_Isotope = get_na_isotope(peptide,charge)\n",
    "\n",
    "    dax_legend_elements = []\n",
    "    for irep in range(max_time_reps):\n",
    "        dax_legend_elements += [ mlines.Line2D([0],[0], color='w',markerfacecolor = mpl_colors[irep],\n",
    "                                              marker='o',label=\"rep\"+str(irep+1),markersize=10) ]\n",
    "    if Test_Data:\n",
    "        dax_legend_elements = [ mlines.Line2D([0],[0], color='w',markerfacecolor = mpl_colors[0],\n",
    "                                              marker='v',label=\"Fit Data\",markersize=10),\n",
    "                                mlines.Line2D([0],[0], color='w',markerfacecolor = None,markeredgecolor='darkred',\n",
    "                                              marker='o',label=\"Solution\",markersize=10) ]\n",
    "    print(\"\\nDataset\",index+1,\"of\",len(metadf))\n",
    "    print(\"Performing fits for \"+sample+\" \"+peptide_range+\": \"+peptide+\" z=\"+str(int(charge)))\n",
    "    #print(n_time_points, max_time_reps) \n",
    "\n",
    "    nrows = n_time_points\n",
    "    if max_time_reps == 1: Overlay_reps = False\n",
    "    if Overlay_reps:\n",
    "        ncols = max_time_reps + 1\n",
    "    else: ncols = max_time_reps\n",
    "    \n",
    "    #figsize is width, height\n",
    "    fig, ax = plt.subplots(figsize=(ncols*5+2, n_time_points*5), ncols=ncols, nrows = nrows, squeeze=False)\n",
    "    if Bootstrap: fig2, ax2 = plt.subplots(figsize=(ncols*5+2, n_time_points*5), ncols=ncols, nrows = nrows, squeeze=False)\n",
    "    if Test_Data: dfig,dax=plt.subplots(figsize=(n_time_points/3+9,6)) # numD vs time plot\n",
    "    else: dfig,dax=plt.subplots(figsize=(9,6))\n",
    "\n",
    "    #time points are rows i, reps are columns j\n",
    "    data_fits = pd.DataFrame()\n",
    "    data_fit = pd.DataFrame()\n",
    "\n",
    "    ## get corrected deut values from centroids (not fit data) of Un and FullDeut\n",
    "    ## Need these to compare to the 'solution' values\n",
    "    ## TODO safety function incase UN or TD are missing \n",
    "    n_amides = count_amides(peptide,count_sc=0.0)\n",
    "    max_n_amides = count_amides(peptide,count_sc=0.5)\n",
    "    Noise = 0.0\n",
    "    d_corr = 1.0\n",
    "    if all(tp in time_points for tp in [0,1e6]):\n",
    "        time_reps = deutdata.rep[(deutdata.time==0.0)].unique().astype('int')\n",
    "        n_time_reps = len(time_reps)\n",
    "        centroidUD_all = []\n",
    "        for r in time_reps:\n",
    "            focal_data = deutdata.copy()[(deutdata.time==0.0) & (deutdata.rep==r)]\n",
    "            mz=np.array(focal_data.mz.copy())\n",
    "            y=np.array(focal_data.Intensity.copy())\n",
    "            centroidUD_all += [sum(mz*y)/sum(y)]\n",
    "            Noise += max(y)/n_time_reps\n",
    "        centroidUD = np.mean(centroidUD_all)\n",
    "        centroidUD_err = np.std(centroidUD_all)\n",
    "\n",
    "        time_reps = deutdata.rep[(deutdata.time==1e6)].unique().astype('int')\n",
    "        n_time_reps = len(time_reps)\n",
    "        centroidTD_all = []\n",
    "        for r in time_reps:\n",
    "            focal_data = deutdata.copy()[(deutdata.time==1e6) & (deutdata.rep==r)]\n",
    "            mz=np.array(focal_data.mz.copy())\n",
    "            y=np.array(focal_data.Intensity.copy())\n",
    "            centroidTD_all += [sum(mz*y)/sum(y)] #total deuteration center\n",
    "            Noise += max(y)/n_time_reps\n",
    "        centroidTD = np.mean(centroidTD_all)\n",
    "        centroidTD_err = np.std(centroidTD_all)    \n",
    "        d_corr = ((centroidTD - centroidUD)/n_amides*charge)\n",
    "        Noise = Noise/2.0 * Y_ERR/100.0 #noise is Y_ERR * avg maxInt of UnDeut and FullDeut\n",
    "    else: \n",
    "        print(\"Missing Un or Full Datasets\\nUsing percentage of maxInt for Noise or setNoise if specified\")\n",
    "        Noise = Y_ERR/100.0 * deutdata.Intensity.max()\n",
    "\n",
    "    if setNoise: Noise = setNoise  \n",
    "\n",
    "    for i in range(0,n_time_points):\n",
    "        n_time_rep = int(max(deutdata.rep[(deutdata.time_idx==i)]))\n",
    "        timept = int(max(deutdata.time[(deutdata.time_idx==i)]))\n",
    "        if timept == int(1e6): \n",
    "            timelabel = 'FullDeut'\n",
    "        elif timept == 0: timelabel = 'UnDeut'\n",
    "        else: timelabel = str(timept)+'s'\n",
    "\n",
    "        ## TODO would like to test n_curves for all reps in time point, then do bootstrap with best_n_curves\n",
    "        for j in range(1,n_time_rep+1):  \n",
    "            lowermz = deutdata.mz[deutdata.rep==j].min()\n",
    "            uppermz = deutdata.mz[deutdata.rep==j].max()\n",
    "\n",
    "            ## these shouldn't be nested in loop, depend only on peptide\n",
    "            ## artefact of treating Test data samples as reps\n",
    "            # n_amides = count_amides(peptide,count_sc=0.0)\n",
    "            # max_n_amides = count_amides(peptide,count_sc=0.5)\n",
    "            # nas = int(len(na_isotope)/n_time_rep)\n",
    "            # Current_Isotope = na_isotope[(j-1)*nas:j*nas]\n",
    "        \n",
    "            focal_data = deutdata.copy()[(deutdata.time_idx == i) & (deutdata.rep == j)]\n",
    "            if focal_data.empty: continue\n",
    "            if Keep_Raw: focal_raw = rawdata.copy()[(rawdata.time_idx == i) & (rawdata.rep == j)]\n",
    "            envelope_height = focal_data['Intensity'].max() * Env_threshold\n",
    "            env, env_Int = get_mz_env(envelope_height,focal_data,pts=True)\n",
    "            \n",
    "            mz=np.array(focal_data.mz.copy())\n",
    "            y=np.array(focal_data.Intensity.copy())\n",
    "            x=np.full(y.shape,len(y))\n",
    "\n",
    "            env_symmetry_adj = 2.0 - (y.max() - env_Int)/y.max() # 0 -> assym, 1 -> symm  \n",
    "                                                          # want 0 to be 2x and 1 to be 1x -> y = -1*x + 2\n",
    "\n",
    "            ynorm_factor = np.sum(y)\n",
    "            y_norm = y / ynorm_factor # 50secs with vs 2 mins without normalization\n",
    "            \n",
    "            if Scale_Y_Values: \n",
    "                scale_y = ynorm_factor\n",
    "            else:\n",
    "                if Keep_Raw: focal_raw.Intensity = rawdata.Intensity / ynorm_factor  #norm raw instead\n",
    "                scale_y = 1.0\n",
    "\n",
    "            n_bins = len(y)-1\n",
    "            \n",
    "            max_y = np.max(y) #for parameter initialization\n",
    "\n",
    "            data_fit =pd.DataFrame({'time':[timept],'rep':[j],'centroid':[sum(mz*y)/sum(y)]})\n",
    "                    \n",
    "            fstdev=[]     \n",
    "            \n",
    "            p_corr = 1.0 \n",
    "            low_n = 1            \n",
    "            if Limit_by_envelope: high_n = min(max(1,int(env_symmetry_adj * charge * (env[1]-env[0])/(3*Env_limit)-2/3)),Max_Pops)\n",
    "            else: high_n = Max_Pops\n",
    "            for n_curves in range( low_n, high_n+1 ):  #[scaler] [n *n_curves] [mu *n_curves] [frac * (n_curves )] )]\n",
    "                initial_estimate, bounds = init_params(n_curves,max_n_amides,max_y,seed=Random_Seed)\n",
    "                if (len(initial_estimate) > n_bins): \n",
    "                    print(f\"attempting to fit more paramaters than data points: time {timelabel} rep {j} N={n_curves} curves\")\n",
    "                    #should be able to exit at this point, haven't updated last fit parameters including p_err\n",
    "                    break # exit the for n_curves loop\n",
    "                \n",
    "                fit, covar = curve_fit( n_fitfunc, n_bins, y_norm, p0=initial_estimate, maxfev=int(1e6), \n",
    "                                        bounds = bounds   )\n",
    "                fit_y = n_fitfunc( n_bins, *fit )\n",
    "                if n_curves == low_n: \n",
    "                    best_fit = fit \n",
    "                    best_covar = covar\n",
    "                # Perform statistical test, keep the best model\n",
    "                n_params = len( initial_estimate )\n",
    "                rss = calc_rss( y_norm, fit_y, )\n",
    "                if n_curves == low_n: print( timelabel +' '+str(sample) +' N = ' + str(n_curves).ljust(5) + 'p = ' + format( p_corr, '.3e')+str(fit),file=fout)\n",
    "\n",
    "                if n_curves > low_n:\n",
    "                    F = ( ( prev_rss - rss ) / 2  ) / ( rss / ( n_bins + 1 - n_params ) )\n",
    "                    p = 1.0 - stats.f.cdf( F, 2, n_bins + 1 - n_params )\n",
    "                    p_corr = p * (n_curves-1)\n",
    "                    print( timelabel +' '+str(sample) +' N = ' + str(n_curves).ljust(5) + 'p = ' + format( p_corr, '.3e')+str(fit),file=fout)\n",
    "                    if p_corr >= Ncurve_p_accept:\n",
    "                        p_corr = prev_pcorr \n",
    "                        break # exit the for n_curves loop; insufficient improvement in fit\n",
    "                    ## fall back to n-1 curves if one of the populations <  Pop_Thresh\n",
    "                    _,_,_,frac_check = get_params(*fit,norm=True,unpack=True)\n",
    "                    if np.min(frac_check) < Pop_Thresh:\n",
    "                        p_corr = prev_pcorr\n",
    "                        print (\"min population below threshold: falling back to\",(n_curves-1),\"curve(s)\")\n",
    "                        break\n",
    "                prev_rss = rss   #only gets to these if N is better than N-1           \n",
    "                prev_pcorr = p_corr\n",
    "                best_fit = fit            \n",
    "                best_covar = covar \n",
    "                best_n_curves = n_curves    \n",
    "            #end n_curves for loop\n",
    "\n",
    "            fit_y = n_fitfunc( n_bins, *best_fit )\n",
    "            fstdev = np.sqrt(np.diag(best_covar)) ### this error is not realistic\n",
    "                                                  ### Artifically small: fit can be very well converged but still miss the mark\n",
    "                                                  ### since measurement contains error and fit is assuming perfect\n",
    "                                                  ### this is why adding bootstrap noise better samples around 'solution'\n",
    "                                                  ### Artifically large: essentially swapping between populations during curve_fit\n",
    "            #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html see pcov\n",
    "            # To compute one standard deviation errors on the parameters, use perr = np.sqrt(np.diag(pcov)) //when sigma=None\n",
    "\n",
    "            #print( timelabel +' '+str(samples[j-1]) +' N = ' + str(best_n_curves).ljust(5) + 'p = ' + format( p_corr, '.3e')+str(best_fit),file=fout)\n",
    "\n",
    "            if Bootstrap:            \n",
    "                p0_boot=[]\n",
    "                for boot in range(Nboot): #send different p0 for each Bootstrap iteration\n",
    "                    if Boot_Seed: bseed = boot+Random_Seed+1\n",
    "                    else: bseed = Random_Seed\n",
    "                    p0, bbounds = init_params(best_n_curves,max_n_amides,max_y,seed=bseed) #best_n_curves\n",
    "                    p0_boot.append(p0)\n",
    "                #p0_boot, bbounds = init_params(best_n_curves,n_amides,max_y,seed=Random_Seed)\n",
    "                #rss = 0.0 #calc_rss( y_norm, fit_y, ) #this doesn't seem appropriate for binom fits            \n",
    "                pfit, perr = fit_bootstrap(p0_boot,bbounds,n_bins,y_norm,sigma_res=Noise/ynorm_factor, \n",
    "                                           nboot=Nboot,ax=ax[i,j-1],full=Full_boot,yscale=scale_y)\n",
    "                #pfit = get_params(*pfit,sort=False,norm=True,unpack=False)\n",
    "                \n",
    "                # plot mus vs fracs for each dataset if Full_boot=True\n",
    "                if Full_boot:\n",
    "                    p_array=np.array(pfit)\n",
    "                    ns_array = p_array[:,1:best_n_curves+1]\n",
    "                    mus_array = p_array[:,best_n_curves+1:best_n_curves*2+1]                \n",
    "                    fracs_array = p_array[:,-best_n_curves:]\n",
    "\n",
    "                    for n in range(best_n_curves):\n",
    "                        nm = mus_array[:,n]*ns_array[:,n]*1/d_corr #apply correction based on TD-UN\n",
    "                        fracn = fracs_array[:,n]\n",
    "                        if best_n_curves > 1:\n",
    "                            nm_avg = nm[(fracn > Pop_Thresh) & (fracn < 1 - Pop_Thresh)].mean()\n",
    "                            nm_err = nm[(fracn > Pop_Thresh) & (fracn < 1 - Pop_Thresh)].std()\n",
    "                            frac_avg = fracn[(fracn > Pop_Thresh) & (fracn < 1 - Pop_Thresh)].mean()\n",
    "                            frac_err = fracn[(fracn > Pop_Thresh) & (fracn < 1 - Pop_Thresh)].std()\n",
    "                        else: \n",
    "                            nm_avg = nm.mean()\n",
    "                            nm_err = nm.std()\n",
    "                            frac_avg = fracn.mean()\n",
    "                            frac_err = fracn.std()\n",
    "                        #ax2[i,j-1].scatter(fracs_array[:,n],mus_array[:,n],label='pop'+str(n))\n",
    "                        ax2[i,j-1].scatter(nm,fracs_array[:,n],label='pop'+str(n),alpha=0.6)\n",
    "                        ax2[i,j-1].errorbar(nm_avg,frac_avg,xerr=nm_err,yerr=frac_err,elinewidth=2,zorder=0,color=mpl_colors_dark[n])\n",
    "                        if Overlay_reps:\n",
    "                            ax2[i,ncols-1].scatter(nm,fracs_array[:,n],alpha=0.6,label=str(j)+'_pop'+str(n),)\n",
    "                            ax2[i,ncols-1].errorbar(nm_avg,frac_avg,xerr=nm_err,yerr=frac_err,elinewidth=2,zorder=0)\n",
    "                        if Test_Data: #plot against time_set not the fake timept value\n",
    "                            dax.errorbar(i,nm_avg,yerr=nm_err,alpha=1.0,color=mpl_colors[j-1])\n",
    "                            dax.scatter(i,nm_avg,marker='v',alpha=1.0,s=100.0*frac_avg+20.0,\n",
    "                                        color=mpl_colors[j-1],edgecolors=mpl_colors_dark[j-1])\n",
    "                        else: \n",
    "                            dax.errorbar(timept,nm_avg,yerr=nm_err,alpha=1.0,color=mpl_colors[j-1])\n",
    "                            dax.scatter(timept,nm_avg,marker='v',alpha=1.0,s=50.0*frac_avg+5.0,\n",
    "                                        color=mpl_colors[j-1],edgecolors=mpl_colors_dark[j-1])\n",
    "                    if Test_Data:\n",
    "                        for k in range(3):\n",
    "                            s_nm = solution['fd'+str(k+1)][solution.time==timept].to_numpy()[0]/100*n_amides #this is #D uncorrected\n",
    "                            s_f = solution['p'+str(k+1)][solution.time==timept].to_numpy()[0]\n",
    "                            ax2[i,j-1].scatter(s_nm,s_f,edgecolors='darkred',alpha=1.0,marker='o',facecolor='none',s=60,linewidth=1.5)\n",
    "                            dax.scatter(i,s_nm,edgecolors='darkred',alpha=1.0,marker='o',facecolor='none',s=100.0*s_f+20.0,)\n",
    "\n",
    "\n",
    "            ### PLOTS ###\n",
    "            y_plot = y_norm * scale_y\n",
    "            fit_y = fit_y * scale_y\n",
    "            scaled_env_height = max(y_plot)*Env_threshold\n",
    "            env_resolution = env_symmetry_adj *charge * (env[1]-env[0]) / ( len(best_fit) + 1.0 ) #rough measure of whether there's enough information to do the n_curves fit\n",
    "            \n",
    "            env_label = \"Env res: \"+format(env_resolution,'0.2f')#+\"/\"+format(env_dof,'0.2f')\n",
    "            ax[i,j-1].plot(env,[scaled_env_height,scaled_env_height],label=env_label,color='darkorange')\n",
    "            if Keep_Raw:\n",
    "                ax[i,j-1].plot( focal_raw.mz, focal_raw.Intensity, color='#999999' ) #ax[i,j-1]\n",
    "            else: ax[i,j-1].vlines( mz, 0.0, y_plot, color='#999999' ) #ax[i,j-1]\n",
    "            \n",
    "            ax[i,j-1].plot( mz, y_plot, 'ro', label='data '+ timelabel+\", rep \"+str(j), markersize='4')\n",
    "            ax[i,j-1].vlines( data_fit.centroid[0], 0, max(y_plot), color='orange' ,label='m/z = '+format(data_fit.centroid[0],'.2f'),linestyles='dashed',linewidth=2)\n",
    "\n",
    "            \n",
    "            ax[i,j-1].plot( mz, fit_y, '-', label='fit sum N='+str(best_n_curves))\n",
    "\n",
    "            if Overlay_reps:\n",
    "                if Keep_Raw:\n",
    "                    ax[i,ncols-1].plot( focal_raw.mz, focal_raw.Intensity, color=mpl_colors[j-1],alpha=0.5 ) \n",
    "                else: ax[i,ncols-1].vlines( mz, 0.0, y_plot, color=mpl_colors[j-1], alpha=0.5 )\n",
    "                ax[i,ncols-1].plot( mz, y_plot, 'o',color=mpl_colors_dark[j-1], label=\"rep \"+str(j), markersize='4')\n",
    "                ax[i,ncols-1].plot( mz, fit_y, '-',color=mpl_colors[j-1])# label='fit sum N='+str(best_n_curves))\n",
    "\n",
    "            scaler,nexs,mus,fracs = get_params(*best_fit,sort=False,norm=False,unpack=True)\n",
    "            scaler = np.power( 10.0, scaler )   \n",
    "            fracsum = np.sum(fracs)\n",
    "\n",
    "            #scaler at 0, n's at 1:best_n_curves+1, mu's at best_n_curves+1:2*best_ncurves+1, fracs at 2*best_n_curves+1:\n",
    "            for k in range( best_n_curves ):\n",
    "                nex = nexs[k]\n",
    "                nex_err = fstdev[k+1]\n",
    "                mu = mus[k]\n",
    "                mu_err = fstdev[k+best_n_curves+1]\n",
    "                frac = fracs[k]/fracsum\n",
    "                frac_err = min(1.0,fstdev[-1]/fracsum) #if best_n_curves > 1 else 0.\n",
    "                fit_yk = scaler * frac * fitfunc( n_bins, nex, mu, ) * scale_y\n",
    "                kindcent = sum(mz*fit_yk)/sum(fit_yk)\n",
    "                data_fit['icentroid_'+str(k+1)]=kindcent\n",
    "                data_fit['ipop_'+str(k+1)]=frac\n",
    "                data_fit['ipop_std_'+str(k+1)]=frac_err\n",
    "                data_fit['imu_'+str(k+1)]=mu\n",
    "                data_fit['iNex_'+str(k+1)]=nex\n",
    "                data_fit['iNex_std_'+str(k+1)]=nex_err\n",
    "                #leaving off errors until I can account for populations swapping during fit\n",
    "                #maybe if fixed mu1 < mu2 or somesuch \n",
    "                plot_label = ('pop'+str(k+1)+' = '+format(frac,'.2f')#+'  '+format(frac_err,'.2f') \n",
    "                                    +'\\nNex'+str(k+1)+' = '+format(nex,'.1f')#+'  '+format(nex_err,'.2f')\n",
    "                                    +'\\nm/z = '+format(kindcent,'.2f'))\n",
    "                ax[i,j-1].plot( mz, fit_yk, color = 'black', linestyle='solid',linewidth=1.,label=plot_label)\n",
    "                ax[i,j-1].set(xlim=(lowermz-3/charge,uppermz+9/charge)) #gives rightside space for legend\n",
    "            ax[i,j-1].set_title(label=str(sample)+': '+peptide_range+\" \"+str(peptide)+\" z=\"+str(int(charge)),loc='center')\n",
    "            ax[i,j-1].legend(frameon=False,loc='upper right');\n",
    "            \n",
    "            if Overlay_reps: \n",
    "                ax[i,ncols-1].set_title(label='Replicates Overlay')\n",
    "                ax[i,ncols-1].set(xlim=(lowermz-3/charge,uppermz+9/charge)) \n",
    "                ax[i,ncols-1].legend(frameon=False,loc='upper right',title='data '+ timelabel);\n",
    "            data_fit['iscaler']=scaler\n",
    "            data_fits = data_fits.append(data_fit, ignore_index=True)\n",
    "            if Bootstrap:\n",
    "                ax2[i,j-1].set_title(label=str(sample)+': '+peptide_range+\" \"+str(peptide)+\" z=\"+str(int(charge)),loc='center')\n",
    "                ax2[i,j-1].legend(title=timelabel+\", rep\"+str(j),frameon=True,loc='upper right');\n",
    "                ax2[i,j-1].set(xlim=(-1.,max_n_amides+4),ylim=(-0.05,1.05))\n",
    "                ax2[i,j-1].set_xlabel(\"N*mu\")\n",
    "                ax2[i,j-1].set_ylabel(\"population\") \n",
    "                if Overlay_reps:\n",
    "                    ax2[i,ncols-1].set(xlim=(-1.,max_n_amides+4),ylim=(-0.05,1.05))\n",
    "                    ax2[i,ncols-1].legend(title=timelabel,frameon=True,loc='upper right');\n",
    "                    ax2[i,ncols-1].set_title(label='Replicates Overlay')\n",
    "                    ax2[i,ncols-1].set_xlabel(\"N*mu\")\n",
    "                    ax2[i,ncols-1].set_ylabel(\"population\") \n",
    "\n",
    "    dax.set_ylabel(\"Relative Deuterium Level (Da)\")\n",
    "    dax.set_title(label=str(sample)+': '+peptide_range+\" \"+str(peptide)+\" z=\"+str(int(charge)),loc='center')\n",
    "    dfig.tight_layout()\n",
    "    if Test_Data: \n",
    "        yds = np.unique(solution[['fd1','fd2','fd3']].dropna().values)*n_amides/100.0\n",
    "        dax.hlines(xmin=[-1.0] * len(yds),xmax=[n_time_points+1.0] * len(yds),y=yds,alpha=0.5,linestyles='dotted',color='grey')\n",
    "        dax.set(xlim=(-0.5,n_time_points+0.5))\n",
    "        dax.set_xlabel(\"Test Experiment\")\n",
    "        dax.legend(handles=dax_legend_elements,loc='upper left')\n",
    "    else:\n",
    "        dax.set_xlabel(\"Time, s\")\n",
    "        dax.set_xscale('log')\n",
    "        dax.legend(handles=dax_legend_elements[0:max_time_reps],loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    if Full_boot: fig2.tight_layout()\n",
    "    \n",
    "\n",
    "    if Bootstrap: \n",
    "        if Y_ERR: yerr_name = 'bootNoise0p'+format(Y_ERR,'.2f')[-2:]+'_'\n",
    "        else: yerr_name = 'NoNoise_'\n",
    "    else: yerr_name = ''\n",
    "    try:\n",
    "        figfile = 'hdx_ms_hxex3_'+sample+peptide_range+fitfunc_name+'_p'+format(int(Ncurve_p_accept*100),'02d')+'_IndFits_'+yerr_name+date\n",
    "        print(\"saving figure as \",figfile)\n",
    "        fig.savefig(os.path.join(Data_DIR,figfile+'.pdf'),format='pdf',dpi=600)\n",
    "        if SVG: fig.savefig(os.path.join(Data_DIR,figfile+'.svg'),format='svg')\n",
    "        if Hide_Figure_Output: plt.close(fig)\n",
    "        else: plt.show()\n",
    "    except IOError as e:\n",
    "        print (f\"Could not save: {figfile} file is open\")   \n",
    "\n",
    "    try:\n",
    "        fig2file = 'hdx_ms_hxex3_'+sample+peptide_range+fitfunc_name+'_p'+format(int(Ncurve_p_accept*100),'02d')+'_BootFits_'+yerr_name+date\n",
    "        print(\"saving figure as \",fig2file)\n",
    "        fig2.savefig(os.path.join(Data_DIR,fig2file+'.pdf'),format='pdf',dpi=600)\n",
    "        if SVG: fig2.savefig(os.path.join(Data_DIR,fig2file+'.svg'),format='svg')\n",
    "        if Hide_Figure_Output: plt.close(fig2)\n",
    "        else: plt.show()\n",
    "    except IOError as e:\n",
    "        print (f\"Could not save: {fig2file} file is open\") \n",
    "\n",
    "    try:\n",
    "        dfigfile = 'hdx_ms_hxex3_'+sample+peptide_range+fitfunc_name+'_p'+format(int(Ncurve_p_accept*100),'02d')+'_ndeutBoot_'+yerr_name+date\n",
    "        print(\"saving figure as \",dfigfile)\n",
    "        dfig.savefig(os.path.join(Data_DIR,dfigfile+'.pdf'),format='pdf',dpi=600)\n",
    "        if SVG: dfig.savefig(os.path.join(Data_DIR,dfigfile+'.svg'),format='svg')\n",
    "        if Hide_Figure_Output: plt.close(dfig)\n",
    "        else: plt.show()\n",
    "    except IOError as e:\n",
    "        print (f\"Could not save: {dfigfile} file is open\") \n",
    "\n",
    "fout.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87069fe9496d8564739158a499e4dbad7723cde4d40959d250a61cc05c84299a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
